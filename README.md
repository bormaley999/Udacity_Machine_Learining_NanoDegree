# Udacity_Machine_Learining_NanoDegree

Welcome to my [Udacity Machine Learning NanoDegree](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t) workbench. I will store my portfolio projects here. 

## Structure 
1. **The IMDb Sentiment Analysis Project Using Pytorch and RNN** could be found [here](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/tree/master/Udacity_Machine_Learining_NanoDegree/imbD%20Sentiment%20Analysis) the reason for folder's devision is simple - each folder represents a standalone project. 
<br>_Short Overview of the Project_<br>
The notebook and Python files provided here, once completed, result in a simple web app which interacts with a deployed recurrent neural network performing sentiment analysis on movie reviews.
Please see the README from Udacity's main folder for instructions on setting up a SageMaker notebook and downloading the project files (as well as the other notebooks).
The link to [README](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/blob/master/Udacity_Machine_Learining_NanoDegree/imbD%20Sentiment%20Analysis/README.md) file about the IMDb Sentiment Analysis Project using PyTorch.


2. **Detecting Payment Card Fraud** could be found [here](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/tree/master/Fraud%20Detection).
<br>_Short Overview of the Project_<br>
In this project, we'll look at a credit card fraud detection dataset, and build a binary classification model that can identify transactions as either fraudulent or valid, based on provided, historical data.
Detailed project description could be found in the Project Notebook in the project folder. 
The link to [README](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/blob/master/Fraud%20Detection/Readme.md).

3. **Moon Data Classification / Deploying Custom PyTorch Model** could be found [here](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/tree/master/Moon%20Data%20Classification%20%5C%20Custom%20PyTorch%20Model).
<br>_Short Overview of the Project_<br>
In this project, we'll employ two, unsupervised learning algorithms to do population segmentation. Population segmentation aims to find natural groupings in population data that reveal some feature-level similarities between different regions in the US.
Detailed project description could be found in the Project Notebook in the project folder. 
The link to [README](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/blob/master/Moon%20Data%20Classification%20%5C%20Custom%20PyTorch%20Model/Readme.md).

4. **Population Segmentation** could be found [here](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/tree/master/Population%20Segmentation%20with%20SageMaker).
The link to [README](https://github.com/bormaley999/Udacity_Machine_Learining_NanoDegree/blob/master/Population%20Segmentation%20with%20SageMaker/Readme.md).
<br>_Short Overview of the Project_<br>
In this project, we'll employ two, unsupervised learning algorithms to do population segmentation. Population segmentation aims to find natural groupings in population data that reveal some feature-level similarities between different regions in the US. Detailed project description could be found in the Project Notebook in the project folder. 



## Dependacies 
Also, it is important to note that in order to run some projects (e.g. IMDb Sentimenent Analysis), I have to turn-on an AWS Sagemaker API which is costly, so I will do that ONLY for an interview or review processes.

If you would like to have fun playing around with this project you have to set up AWS instance for yourself. The description of how to do that is presented in the README file above. Alternatively, you could check sagemaker-deployment (https://github.com/udacity/sagemaker-deployment) folder I forked from Udacity.

Additionaly, pay attention that in order to reproduce the code from the Notebooks, you have to refer to the original datasets presented in the Readme files of each project.

Thank you for taking the time to read this and I hope it was useful for you.
