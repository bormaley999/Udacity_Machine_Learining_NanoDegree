{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The strings() method of twitter_samples will print all of the tweets within a dataset as strings. \n",
    "# Setting the different tweet collections as a variable will make processing and testing easier.\n",
    "\n",
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "\n",
    "# Using NLTK’s tokenizers such as punkt\n",
    "# The punkt module is a pre-trained model that helps tokenize words and sentences. \n",
    "tweet_tokens = twitter_samples.tokenized(\"positive_tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweet example: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive tweet example:\", positive_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative tweet example: hopeless for tmr :(\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative tweet example:\", negative_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets is: 5000\n",
      "The number of negative tweets is: 5000\n",
      "The dataset is consisting of: 20000\n"
     ]
    }
   ],
   "source": [
    "# Dataset details\n",
    "\n",
    "print(\"The number of positive tweets is:\", len(positive_tweets))\n",
    "print(\"The number of negative tweets is:\", len(negative_tweets))\n",
    "print(\"The dataset is consisting of:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    \"\"\"\n",
    "    This function removes noise and incorporates the normalization and lemmatization functions. \n",
    "    The code takes two arguments: the tweet tokens and the tuple of stop words.\n",
    "    \n",
    "    Normalization Part of the function\n",
    "    The code then uses a loop to remove the noise from the dataset. \n",
    "    To remove hyperlinks, the code first searches for a substring that matches a URL starting with \n",
    "    http:// or https://, followed by letters, numbers, or special characters. \n",
    "    Once a pattern is matched, the .sub() method replaces it with an empty string, or ''.\n",
    "    \n",
    "    Similar approach applied to @ signs.\n",
    "    Punctuation using the library string.\n",
    "    \n",
    "    Lemmatization part of the function\n",
    "    The function lemmatize_sentence first gets the position tag of each token of a tweet. \n",
    "    Within the if statement, if the tag starts with NN, the token is assigned as a noun. \n",
    "    Similarly, if the tag starts with VB, the token is assigned as a verb.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# testing .words() method\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(remove_noise(tweet_tokens[0], stop_words)) # because we have deployed remove_noise function above\n",
    "                                                   # all @ mentions, stop words, and converts the words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing code to clean the sample tweets\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Testing the implementaion of the code which cleaning the sample positive tweets\n",
    "print(positive_tweet_tokens[0])\n",
    "print(positive_cleaned_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hopeless', 'for', 'tmr', ':(']\n",
      "['hopeless', 'tmr', ':(']\n"
     ]
    }
   ],
   "source": [
    "# Testing the implementaion of the code which cleaning the sample negative tweets\n",
    "print(negative_tweet_tokens[0])\n",
    "print(negative_cleaned_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    \"\"\"\n",
    "    This function takes a list of tweets as an argument to provide a list of words \n",
    "    in all of the tweet tokens joined.\n",
    "    \"\"\"\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "# Finding out the most common words in the postive cleaned sample tweets, \n",
    "# using the FreqDist class of NLTK\n",
    "# The .most_common() method lists the words which occur most frequently in the data.\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('miss', 291), ('go', 275), ('please', 275), ('want', 246), ('like', 218)]\n"
     ]
    }
   ],
   "source": [
    "# Finding out the most common words in the negative cleaned sample tweets, \n",
    "# using the FreqDist class of NLTK\n",
    "# The .most_common() method lists the words which occur most frequently in the data.\n",
    "\n",
    "freq_dist_net = FreqDist(all_neg_words)\n",
    "print(freq_dist_net.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Tokens to a Dictionary\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    \"\"\"\n",
    "    This functions converts the tweets from a list of cleaned tokens \n",
    "    to dictionaries with keys as the tokens and True as values. \n",
    "    \"\"\"\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset for Training and Testing the Model\n",
    "# This code attaches a Positive or Negative label to each tweet. \n",
    "# It then creates a dataset by joining the positive and negative tweets.\n",
    "\n",
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# To avoid bias, I’ve added code to randomly arrange the data using \n",
    "# the .shuffle() method of random.\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# The number of tweets is 10000\n",
    "# The code splits the shuffled data into a ratio of 70:30 for training and testing\n",
    "train_data = dataset[:7000]\n",
    "test_data  = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9946666666666667\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =   1002.1 : 1.0\n",
      "                     sad = True           Negati : Positi =     55.8 : 1.0\n",
      "                follower = True           Positi : Negati =     33.6 : 1.0\n",
      "                     bam = True           Positi : Negati =     22.3 : 1.0\n",
      "                    glad = True           Positi : Negati =     18.3 : 1.0\n",
      "                followed = True           Negati : Positi =     15.8 : 1.0\n",
      "                 welcome = True           Positi : Negati =     15.8 : 1.0\n",
      "                 forward = True           Positi : Negati =     14.3 : 1.0\n",
      "                    blog = True           Positi : Negati =     14.3 : 1.0\n",
      "                     ugh = True           Negati : Positi =     13.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Accuracy is defined as the percentage of tweets in the testing dataset for which the model was correctly able to predict the sentiment. A 99.46% accuracy on the test set is quite good indicator.\n",
    "\n",
    "In the table that shows the most informative features, every row in the output shows the ratio of occurrence of a token in positive and negative tagged tweets in the training dataset. The first row in the data signifies that in all tweets containing the token :(, the ratio of negative to positives tweets was 1002.1 to 1. Interestingly, it seems that there was one token with :( in the positive datasets. You can see that the top two discriminating items in the text are the emoticons. Further, words such as sad lead to negative sentiments, whereas follower and bam are associated with positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Checking how the model performs on random tweets.\n",
    "# custom_tweet variable could be edited to check the model performance.\n",
    "# This code is updating the string associated with the custom_tweet variable.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I ordered from AliExpress last week. They delayed my order for a week! I will never this service in the future.\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
